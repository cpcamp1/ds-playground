{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week10 Classification & Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We continued learning other algorithms of Supervised Learning. Classification belongs to the category of supervised learning where the targets also provided with the input data. \n",
    "\n",
    "Classification is the process of predicting the class of given data points. Classes are sometimes called as targets/ labels or categories. Classification predictive modeling is the task of approximating a mapping function (f) from input variables (X) to discrete output variables (y).\n",
    "\n",
    "A decision tree is a largely used non-parametric effective machine learning modeling technique for regression and classification problems. To find solutions a decision tree makes sequential, hierarchical decision about the outcomes variable based on the predictor data.\n",
    "\n",
    "In week10, we've covered:\n",
    "* **Logistic Regression, Stochastic Gradient Descent**\n",
    "* **Decision Tree**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best way to consolidate the knowledge in your mind is by practicing.<br>Please complete the part marked with <span style=\"color:green\">**# TODO**</span>.\n",
    "\n",
    "[Google](www.google.com), [Python Documentation](https://docs.python.org/3/contents.html), and [scikit-learn Documentation](https://scikit-learn.org/stable/) are your good friends if you have any python questions.\n",
    "\n",
    "Download **Week10_classification-and-decision-tree.ipynb** notebook to your local folder and open it with Jupyter Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading and EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install Python SciPy libraries (ex. `numpy`, `pandas`, `matplotlib`, `seaborn`)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes to instructors, I found that I needed to do the following to make some pieces of this work:\n",
    "\n",
    "* brew install libomp\n",
    "* brew install graphviz\n",
    "* L1 and L2 logistc each need solvers to complete their work. If you don't specify, a solver will be called that is not implements for L1. See the notes below for a solver that works for both.\n",
    "* I think the GraphViz visualization of the decision tree looks great, but we don't have much indication of that for the participants. Maybe include this in breakouts?\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import graphviz\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, KFold\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "wine = pd.read_csv('winequality-red.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarize the dataset -- we are going to take a look at the data in a few different ways:\n",
    "1. Dimensions of the dataset (Hint: use `shape`)\n",
    "2. Peek at the data itself (Hint: use `head`)\n",
    "3. Statistical summary of all attributes (Hint: use `describe`)\n",
    "4. Breakdown of the data by the class variable (Hint: use `groupby`, `value_counts`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1382\n",
       "1     217\n",
       "Name: quality, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO\n",
    "wine.shape\n",
    "wine.head()\n",
    "wine.describe()\n",
    "wine.groupby(\"quality\").mean()\n",
    "wine['quality'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data visualization -- use seaborn to plot graphs to help you better understand this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='quality', ylabel='alcohol'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO\n",
    "fig = plt.figure(figsize = (10,6))\n",
    "sns.barplot(x = 'quality', y = 'alcohol', data = wine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for missing values, do you think this step is necessary and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fixed acidity           0\n",
       "volatile acidity        0\n",
       "citric acid             0\n",
       "residual sugar          0\n",
       "chlorides               0\n",
       "free sulfur dioxide     0\n",
       "total sulfur dioxide    0\n",
       "density                 0\n",
       "pH                      0\n",
       "sulphates               0\n",
       "alcohol                 0\n",
       "quality                 0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO\n",
    "wine.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you think of some ways to handle missing values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# missing value imputation with mean or 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seperate the dataset as response variable and feature variabes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "X = wine.drop('quality', axis = 1)\n",
    "y = wine['quality']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split to training set and testing set (Note: test_size = 0.25, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply Standard scaling to get optimized result, think of why we need to do this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a Binary Classifier and fit with training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform predictions on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todo\n",
    "y_pred = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've learned that there are some caveats in the train/test split method. In order to avoid this, we can perform something called cross validation. Try K-Fold Cross Validation (ex. 10-fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.825     , 0.88333333, 0.88333333, 0.9       , 0.85833333,\n",
       "       0.84166667, 0.875     , 0.9       , 0.85833333, 0.94107647])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Todo\n",
    "kfold = KFold(n_splits=10)\n",
    "cv_result = cross_val_score(lr,X_train,y_train, cv = kfold,scoring = \"accuracy\")\n",
    "cv_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out Confusion Matrix and explain the Four Quadrants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[336,  10],\n",
       "       [ 37,  16]], dtype=int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Todo\n",
    "confusion_matrix(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's AUC for your Logistic Regression model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6350932521341961"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Todo\n",
    "roc_auc_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regularized Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a Binary Classifier with a L2 Regularizer and print out its Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: For both questions that follow, use solver = 'liblinear' \n",
    "# More on why here at the docs: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[336,  10],\n",
       "       [ 37,  16]], dtype=int64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Todo\n",
    "l2=LogisticRegression(penalty='l2',solver = 'liblinear')\n",
    "l2.fit(X_train, y_train)\n",
    "confusion_matrix(y_test,l2.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a Binary Classifier with a L1 Regularizer and print out its Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[338,   9],\n",
       "       [ 37,  16]], dtype=int64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Todo\n",
    "l1=LogisticRegression(penalty='l1',solver = 'liblinear')\n",
    "l1.fit(X_train, y_train)\n",
    "confusion_matrix(y_test,l1.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the difference between L1 regularization and L2 regularization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 Logistic Regression AUC: 0.635093\n",
      "L1 Logistic Regression AUC: 0.637975\n"
     ]
    }
   ],
   "source": [
    "# Todo\n",
    "print('L2 Logistic Regression AUC: %f' %roc_auc_score(y_test,l2.predict(X_test)))\n",
    "print('L1 Logistic Regression AUC: %f' % roc_auc_score(y_test,l1.predict(X_test)))\n",
    "# different regularizers, L1 often results in many weights being exactly 0 while L2 just makes them small but nonzero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit a SGD model and perform predictions on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todo\n",
    "sgd=SGDClassifier(random_state=0)\n",
    "sgd.fit(X_train, y_train)\n",
    "y_pred_sgd = sgd.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a Decision Tree Classifier and perform predictions on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "tree=DecisionTreeClassifier(random_state=0)\n",
    "tree.fit(X_train, y_train)\n",
    "y_pred_tree_test = tree.predict(X_test)\n",
    "y_pred_tree_train = tree.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[322,  25],\n",
       "       [ 23,  30]], dtype=int64)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test set\n",
    "confusion_matrix(y_test,y_pred_tree_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1035,    0],\n",
       "       [   0,  164]], dtype=int64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#training set\n",
    "confusion_matrix(y_train,y_pred_tree_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is there an overfitting issue? How can you tell and how could you solve it?\n",
    "\n",
    "***Yes, model never makes a mistake on the training set. We should be training with cross validation.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "features = list(X.columns)\n",
    "dot_data=export_graphviz(tree,feature_names = features,filled=True, rounded=True, impurity=False)\n",
    "#graph = graphviz.Source(dot_data)\n",
    "#graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2160 candidates, totalling 10800 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(max_leaf_nodes=3, min_samples_leaf=5, random_state=0)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {'max_leaf_nodes': list(range(2, 50)),\n",
    "          'min_samples_split': [2, 3, 4],\n",
    "          'min_samples_leaf': list(range(5, 20))}\n",
    "grid_cv=GridSearchCV(DecisionTreeClassifier(random_state=0),params,n_jobs=-1,verbose=1)\n",
    "grid_cv.fit(X_train, y_train)\n",
    "grid_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How would you evaluate and compare model performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression: 87.66176470588235\n",
      "SGDClassifier: 85.24369747899159\n",
      "Naive Bayes: 83.48459383753502\n",
      "Decision Tree: 88.32773109243698\n",
      "Random Forest: 88.99579831932775\n",
      "XGBoost: 90.66386554621847\n",
      "AdaBoostClassifier: 87.82773109243698\n",
      "GradientBoostingClassifier: 89.32843137254902\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "models = []\n",
    "\n",
    "models.append((\"Logistic Regression:\",LogisticRegression()))\n",
    "models.append((\"SGDClassifier:\",SGDClassifier()))\n",
    "models.append((\"Naive Bayes:\",GaussianNB()))\n",
    "models.append((\"Decision Tree:\",DecisionTreeClassifier()))\n",
    "models.append((\"Random Forest:\",RandomForestClassifier(n_estimators=7)))\n",
    "models.append((\"XGBoost:\",XGBClassifier(use_label_encoder=False)))\n",
    "models.append((\"AdaBoostClassifier:\",AdaBoostClassifier()))\n",
    "models.append((\"GradientBoostingClassifier:\",GradientBoostingClassifier()))\n",
    "\n",
    "results = []\n",
    "names = []\n",
    "for name,model in models:\n",
    "    kfold = KFold(n_splits=10)\n",
    "    cv_result = cross_val_score(model,X_train,y_train, cv = kfold,scoring = \"accuracy\")\n",
    "    names.append(name)\n",
    "    results.append(cv_result)\n",
    "for i in range(len(names)):\n",
    "    print(names[i],results[i].mean()*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission\n",
    "\n",
    "Commit your completed **Week10_classification-and-decision-tree.ipynb** notebook to your personal Github repo you shared with the faculty."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
